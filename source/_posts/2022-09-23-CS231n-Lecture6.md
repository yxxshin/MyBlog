---
layout: post
title: CS231n Lecture6 정리
description: 'Lecture 6: Training Neural Networks, Part I'
date: '2022-09-23 15:00:00 +0900'
categories:
  - Study
  - CS231n
tags:
  - CS231n
  - ML/AI
toc: true
cover: /images/Stanford_Cover.png
thumbnail: /images/Stanford_Thumbnail.png
noindex: false
sitemap: true
---

# Lecture 6 : Training Neural Networks, Part I
Neural Network를 Train 할 수 있는 다양한 방법들에 대하여 설명한다. Activation Functions, Weight Initialization, Batch Normalization, Babysitting the Learning Process, Hyperparameter Optimization 를 다룬다. 

<!-- more -->

## Activation Functions
다음과 같은 Activation Function에 대하여 논해보자.
 <img src='../../../../images/post/CS231n_Lecture6_Img1.jpg' alt='Image1: Activation Functions' style="display: block; margin: 0 auto"> </img>

### Sigmoid Function
 <img src='../../../../images/post/CS231n_Lecture6_Img2.jpg' alt='Image2: Sigmoid Function' style="display: block; margin: 0 auto; width: 40% ; height: 40%"> </img>

Sigmoid Function은 뉴런의 '활성화'를 가장 직관적으로 잘 나타내어 예전부터 사용되었는데, 다음의 문제점들을 가진다. 

1) **Saturated Neurons "kill" the gradients**
- input이 가령 조금 크거나 (10 이상) 조금 작다면 (-10 이하), 기울기 값이 0이 된다 (saturated). 때문에 Chain Rule을 생각하면 이후에 등장하는 '모든' gradient 값이 0이 되며, 이를 'killing the gradient'라고 표현한다. 

2) **Not zero-centered**
- Zero-Centered는 activation function이 가져야 하는 중요한 조건 중 하나이다. Sigmoid Function의 경우에는 output이 항상 양수(positive)이다. 따라서 Backpropagation 단계 (Backward Flow)를 생각하면 들어오는 gradient 값과 나가는 gradient 값의 부호가 항상 동일하다. 즉, gradient 값들이 all positive 하거나 all negative 하다. 이 경우에는 다음과 같이 zig zag path로 최적화가 진행되므로, 굉장히 비효율적일 확률이 높다.
 <img src='../../../../images/post/CS231n_Lecture6_Img3.jpg' alt='Image3: Zero-centered 하지 않을 경우 발생하는 inefficient update 문제 (zig zag path)' style="display: block; margin: 0 auto; width: 40% ; height: 40%"> </img>

3) exp() is computationally expensive

위의 $tanh(x)$ function은 zero centered 라는 점에서 sigmoid function 보다 좋지만, saturated 되었을 때 발생하는 gradient killing 문제는 해결되지 않았다. 

### ReLU Function (Rectified Linear Unit)
 <img src='../../../../images/post/CS231n_Lecture6_Img4.jpg' alt='Image4: ReLU Function' style="display: block; margin: 0 auto; width: 40% ; height: 40%"> </img>

ReLU function은 + region 에서 saturate 되지 않고, very computationally efficient 하며, sigmoid와 tanh에 비해 빨리 수렴한다(약 6배)는 장점이 있지만, zero-centered 하지 않다. 절반의 영역(- region)에서 saturate 되므로, 아주 작은 양의 bias를 통해 ReLU neuron을 initialize 하는 것이 좋다. 

ReLU function의 문제들을 극복하기 위하여 Leaky ReLU(saturate 하지 않으며, 따라서 뉴런이 "죽지" 않는다), ELU(Leaky ReLU에 비해 노이즈에 강하지만 computationally expensive한 exp 연산이 필요함) 등을 이용한다.

### 요약
- **ReLU** 를 이용하라. (learning rate의 선정에 유의한다)
- **Leaky ReLU, Maxout, ELU**를 시도해봐라.
- tanh 를 시도해 보되, 큰 기대는 하지 마라.
- **Sigmoid는 사용하지 마라**

## Weight Initialization
Fully Connected Net 에서 등장하는 weight $W$ 는 모델이 학습을 통하여 최적의 값을 정해 나간다(backprop). 그렇다면, 처음에는 어떤 값들로 초기화를 해 주어야 하는가?

Idea 1 ) **W = 0** init
모든 값들을 0으로 초기화하는 방법은 어떨까. 결론적으로, 이는 **절대 하지 말아야 하는 방법**이다. 이 경우에는 
