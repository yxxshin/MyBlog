---
layout: post
title: CS231n Lecture7 정리
description: 'Lecture 7: Training Neural Networks, Part II'
date: '2022-11-15 15:00:00 +0900'
categories:
  - Study
  - CS231n
tags:
  - CS231n
  - ML/AI
toc: true
cover: /images/Stanford_Cover.png
thumbnail: /images/Stanford_Thumbnail.png
noindex: false
sitemap: true
---

# Lecture 7 : Training Neural Networks, Part II
<!-- TODO -->

<!-- more -->

## Optimization
SGD보다 나은 optimization 방법들에 대하여 알아본다.

### Problems of SGD
1) loss가 한 쪽 방향으로는 빠르게 변하지만 한 쪽 방향으로는 느리게 변한다면?
 <img src='../../../../images/post/CS231n_Lecture7_Img1.jpg' alt='Image1: Bad situation of SGD' style="display: block; margin: 0 auto; width: 40% ; height: 40%"> </img>
  굉장히 **비효율적인 탐색**.

2) loss function이 *local minima*나 *saddle point*를 가진다면?
  Gradient가 작은 지점들이므로, 그 지점에 '빠지게' 된다. (**Gradient descent gets stuck**). 참고로, high dimension 일수록 saddle point가 매우 흔해진다. 

3) gradient가 minibatch 들로부터 계산되므로 굉장히 **noisy** 할 수 있다.


### Momentum
<img src='../../../../images/post/CS231n_Lecture7_Img2.jpg' alt='Image2: Idea of Momentum' style="display: block; margin: 0 auto; width: 40% ; height: 40%"> </img>

왼쪽 그림과 같이 SGD에 momentum을 도입하는 아이디어가 제시되었다. 오른쪽 그림은 **Nesterov Momentum**에 관한 그림이다. 식으로 표현하면 다음과 같다:

$$ v_{t+1} = \rho v_t - \alpha \nabla f(x_t + \rho v_t) $$
$$ x_{t+1} = x_t + v_{t+1} $$

$x_t$와 $\nabla f(x_t)$ 에 대한 식으로 표현하기 위해서, $\tilde{x}_t = x_t + \rho v_t$ 로 두고 식을 정리하면 아래와 같다.

$$ v_{t+1} = \rho v_t - \alpha \nabla f(\tilde{x}_t) $$
$$ \tilde{x}_{t+1} = \tilde{x}_t - \rho v_t + (1 + \rho)v_{t+1} $$ 
$$ \quad \quad \quad = \tilde{x}_t + v_{t+1} + \rho (v_{t+1} - v_t)$$

### AdaGrad
AdaGrad는 매 단계에서 각 dimension의 historical sum of squares를 바탕으로 한, gradient의 element-wise scaling을 더하는 것이다. (Added element-wise scaling of the gradient based on the historical sum of squares in each dimension) 이미 많이 변화한 변수들은 optimum에 거의 도달했다고 생각하고 step size를 줄이고, 그렇지 않은 경우는 step size를 크게 하기 위해서이다. 

$$ h \leftarrow h + {\partial L \over \partial W} \odot {\partial L \over \partial W} $$
$$ W \leftarrow W - \eta \cdot {1 \over \sqrt{h}} {\partial L \over \partial W} $$

AdaGrad의 문제점은, step size가 점점 작아지기 때문에 stuck 할 수 있다는 것이다.

### RMSProp
위의 문제점을 보완하기 위하여, $h$ 값을 계산할 때 decay rate를 준다., 즉, 과거의 정보에 가중치를 작게 부여하고 최근 값에 큰 가중치를 부여한다. 

$$ h_i \leftarrow \rho \cdot h_{i-1} + (1-\rho)\cdot {\partial L_i \over \partial W} \odot {\partial L_i \over \partial W} $$

### Adam
Adam은 *momentum이 있는 RMSProp* 이라고 생각할 수 있다. 한 특징은, AdaGrad과 Momentum 방식에서 정의한 $h$, $v$가 최초 0으로 설정되어 학습 초반에 0으로 biased 되는 문제를 해결하였다는 점이다. (초반에 너무 큰 보폭 방지)

$$ m_i \leftarrow \beta_{1} \cdot m_{i-1} + (1-\beta_{1}) \cdot {\partial L \over \partial W}$$
$$ \hat{m}_i \leftarrow {m_i \over 1 - \beta_{1}^{i}} $$
$$ v_i \leftarrow \beta_{2} \cdot v_{i-1} + (1-\beta_{2}) \cdot ({\partial L \over \partial W} \odot {\partial L \over \partial W}) $$
$$ \hat{v}_i \leftarrow {v_i \over 1 - \beta_{2}^{i}} $$
$$ \text{next W} = \text{W} - { \text{learning rate} \times \hat{m}_i \over \sqrt{\hat{v}_i} + \text{epsilon}}$$

$\beta_{1} = 0.9$, $\beta_{2} = 0.999$, $\text{learning rate} = 1e-3 \ \ \text{or} \ \ 5e-4 $ 가 많은 모델의 좋은 starting point라고 한다. 

### Learning rate decay
위에서 제시한 SGD, SGD + Momentum, AdaGrad, RMSProp, Adam 모두 learning rate를 hyperparameter로 가진다. 이 때, learning rate에 다음과 같은 decay 방식들을 사용해 줄 수 있다.
- step decay : 몇 개의 epoch 마다 얼마 정도(ex. 절반)씩 learning rate를 낮춘다.
- exponential decay : $ \alpha = \alpha_{0} e^{-kt} $
- 1/t decay : $ \alpha = \alpha_{0}/(1+kt) $

learning rate decay는 second-order hyperparamter라 처음부터 optimize할 필요는 없다. 이 효과는 SGD+Momentum에 중요하고, Adam에서는 자주 쓰이지는 않는다.


### First-Order Optimization


### 

참고자료
https://dalpo0814.tistory.com/29 