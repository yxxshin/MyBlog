---
layout: post
title: CS231n Lecture7 정리
description: 'Lecture 7: Training Neural Networks, Part II'
date: '2022-11-15 15:00:00 +0900'
categories:
  - Study
  - CS231n
tags:
  - CS231n
  - ML/AI
toc: true
cover: /images/Stanford_Cover.png
thumbnail: /images/Stanford_Thumbnail.png
noindex: false
sitemap: true
---

# Lecture 7 : Training Neural Networks, Part II
<!-- TODO -->

<!-- more -->

## Optimization
SGD보다 나은 optimization 방법들에 대하여 알아본다.

### Problems of SGD
1) loss가 한 쪽 방향으로는 빠르게 변하지만 한 쪽 방향으로는 느리게 변한다면?
 <img src='../../../../images/post/CS231n_Lecture7_Img1.jpg' alt='Image1: Bad situation of SGD' style="display: block; margin: 0 auto; width: 40% ; height: 40%"> </img>
  굉장히 **비효율적인 탐색**.

2) loss function이 *local minima*나 *saddle point*를 가진다면?
  Gradient가 작은 지점들이므로, 그 지점에 '빠지게' 된다. (**Gradient descent gets stuck**). 참고로, high dimension 일수록 saddle point가 매우 흔해진다. 

3) gradient가 minibatch 들로부터 계산되므로 굉장히 **noisy** 할 수 있다.


### Momentum
<img src='../../../../images/post/CS231n_Lecture7_Img2.jpg' alt='Image2: Idea of Momentum' style="display: block; margin: 0 auto; width: 40% ; height: 40%"> </img>

왼쪽 그림과 같이 SGD에 momentum을 도입하는 아이디어가 제시되었다. 